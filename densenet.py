# -*- coding: utf-8 -*-
"""googlenet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z4j9KzTjoaMpZkzFqeehBRaRROV8BPm5
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import torchvision



# CIFAR 10 Test dataset and dataloader declaration
project_path = 'content/drive/My Drive/Colab Notebooks/Cybersecurity/NN Attacks/'
# Define a transform to normalize the data
transform = transforms.Compose([transforms.Resize(224),
                                transforms.ToTensor(),
                                transforms.Normalize((0.5,), (0.5,))])
# download CIFAR 10 training set
trainset = torchvision.datasets.CIFAR10(root= project_path+'/data', train=True,
                                        download=True, transform=transform)

# load the trainning set
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html 



# Load the pretrained model
AlexNet_model = torchvision.models.densenet121(pretrained=True)
print(AlexNet_model)
# model = Net()

#change their output size
AlexNet_model.classifier.out_features = 10

#train Alexnet again.

# define the criterion
criterion = nn.CrossEntropyLoss()
# define an SGD optimizer with a lr = 0.001
optimizer = optim.SGD(AlexNet_model.parameters(), lr=0.001)

for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = torch.log_softmax(AlexNet_model(inputs),dim=1) # make a prediction: forward prop
        
        loss = criterion(outputs, labels) # calculate the loss
        
        loss.backward() # calculate gradients
        
        optimizer.step() # updaate weights in backprop

        # print statistics
        running_loss += loss.item()
       
        
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training the Target model')
torch.save(AlexNet_model, 'Googlenet_CIFAR_10.pkl')
