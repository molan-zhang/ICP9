# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hhRbNRxCNmwWHJ19FOxZwnYXwEs5MHs8
"""

# assignment attack CIFAR-10. 
#The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes.
#The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

#import necessary packets

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import torchvision
import sys


n_epochs = 30
batch_size = 128
lr = 0.0001
criterion = nn.CrossEntropyLoss()

# CIFAR 10 Test dataset and dataloader declaration
project_path = 'content/drive/My Drive/Colab Notebooks/Cybersecurity/NN Attacks/'
# Define a transform to normalize the data
transform = transforms.Compose([transforms.Resize(224),
                                transforms.ToTensor(),
                                transforms.Normalize((0.5,), (0.5,))])
# download CIFAR 10 training set
trainset = torchvision.datasets.CIFAR10(root= project_path+'/data', train=True,
                                        download=True, transform=transform)

# load the trainning set
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)

# download the test data
testset = torchvision.datasets.CIFAR10(root=project_path+'/data', train=False,
                                       download=True, transform=transform)

# load the test data
testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html

# helper function to unnormalize and plot image 
def imshow(img):
    img = np.array(img)
    img = np.moveaxis(img, 0, -1)
    plt.imshow(img)
    
# display sample from dataset 
imgs,labels = iter(testloader).next()
imgs = imgs.reshape(3,224, 224)
imshow(imgs)
plt.show()

# Load the pretrained models

Alexnet = torch.load("C:/Users/molan/Downloads/VGG_CIFAR_10.pkl")
# look through their input and output size.
print(Alexnet)

# FGSM attack code
def fgsm_attack(image, epsilon, data_grad):
  # Collect the element-wise sign of the data gradient
  sign_data_grad = data_grad.sign()

  # Create the perturbed image by adjusting each pixel of the input image
  perturbed_image = image + epsilon*sign_data_grad

  # Adding clipping to maintain [0,1] range
  perturbed_image = torch.clamp(perturbed_image, 0, 1)
  # Return the perturbed image
  return perturbed_image

def test(model, test_loader, epsilon):
  model.eval()
  test_loss = 0
  correct = 0
  classnum = 10
  tp = 0
  fn = 0
  fp = 0
  adv_examples = []
   
  # Loop over all examples in test set
  for batch_idx, (inputs, targets) in enumerate(testloader):
    inputs.requires_grad = True
    outputs = model(inputs)
    _, predicted = torch.max(outputs.data, 1)
    # If the initial prediction is wrong, dont bother attacking, just move on
    if predicted.item() != targets.item():
      continue
    loss = criterion(outputs, targets)
    # loss is variable , if add it(+=loss) directly, there will be a bigger ang bigger graph.
    test_loss += loss.item()
    # Zero all existing gradients
    model.zero_grad()
    # Calculate gradients of model in backward pass
    loss.backward()
    # Collect datagrad
    data_grad = inputs.grad.data
    # Call FGSM Attack
    perturbed_data = fgsm_attack(inputs, epsilon, data_grad)
    # Re-classify the perturbed image
    output = torch.softmax(model(perturbed_data),dim=1)

    # Check for success
    final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
    if final_pred.item() < 5 :
        if final_pred.item() == targets.item():
            correct += 1
            tp += 1
        else:
            fp += 1
    else:
        if final_pred.item() == targets.item():
            correct += 1
        if targets.item() < 5:
            fn += 1
                  
    # Special case for saving 0 epsilon examples
    if (epsilon == 0) and (len(adv_examples) < 5):
        adv_ex = perturbed_data.squeeze().detach()
        adv_examples.append( (classes[predicted.item()], classes[final_pred.item()], adv_ex) )
    else:
        # Save some adv examples for visualization later
        if len(adv_examples) < 5:
            adv_ex = perturbed_data.squeeze().detach()
            adv_examples.append((classes[predicted.item()], classes[final_pred.item()], adv_ex) )

  # Calculate final accuracy for this epsilon
  recall = tp/(tp+fn)
  precision = tp/(tp+fp) 
  final_acc = correct/float(len(test_loader))
  print("Epsilon: {}\tTest Accuracy = {} / {} = {}\tRecall = {}\tPrecision = {}".format(epsilon, correct, len(test_loader), final_acc, recall, precision))
  # Return the accuracy and an adversarial example
  return final_acc, adv_examples

accuracies = []
examples = []
epsilons = [0, .05, .1, .15, .2, .25, .3]
for eps in epsilons:
  acc, ex = test(Alexnet, testloader, eps)
  accuracies.append(acc)
  examples.append(ex)
plt.figure(figsize=(5,5))
plt.plot(epsilons, accuracies, "*-")
plt.yticks(np.arange(0, 1.1, step=0.1))
plt.xticks(np.arange(0, .35, step=0.05))
plt.title("Accuracy vs Epsilon for VGG11")
plt.xlabel("Epsilon")
plt.ylabel("Accuracy")
plt.show()
# Plot several examples of adversarial samples at each epsilon
cnt = 0
plt.figure(figsize=(8,10))
for i in range(len(epsilons)):
  for j in range(len(examples[i])):
    cnt += 1
    plt.subplot(len(epsilons),len(examples[0]),cnt)
    plt.xticks([], [])
    plt.yticks([], [])
    if j == 0:
      plt.ylabel("Eps: {}".format(epsilons[i]), fontsize=14)
    orig,adv,ex = examples[i][j]
    plt.title("{} -> {}".format(orig, adv))
    imshow(ex.reshape(3,224,224))
plt.tight_layout()
plt.show()
